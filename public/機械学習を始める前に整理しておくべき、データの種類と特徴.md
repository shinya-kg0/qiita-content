---
title: 機械学習を始める前に整理しておくべき、データの種類と特徴
tags:
  - '機械学習'
  - "データ"
private: false
updated_at: ''
id: null
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

機械学習と聞くと、

・最新のモデルを使って予測する！
・いろんなアルゴリズムを試して精度を上げる！
・チャットボットで業務効率化！

などが思いつくかもしれません。

ただ、機械学習を学んでいく中で面白い格言と出会うでしょう。

> Garbage in, garbage out （ゴミを入れたら、ゴミが出てくる）
> コンピュータ分野でよく言われる格言です。

機械学習モデルというのは、学習するためにデータを使います。
そもそもそのデータの質が悪いと、モデルの質も悪くなるということも表しています。

どのアルゴリズムがいいか、どんな学習をさせたらいいかに目が行きがちですが、
学習させるデータも同じように大事ということです。

入力（学習）するデータは大事なのはわかったけど、どんな特徴があるんですか？
という方に向けてこの記事では全体像を整理しようと思います。

ざっくりデータってこんなものがあるのかと理解していただけると幸いです。


# 1. データソース

まず押さえておきたいのは、「どこから来たデータなのか」です。

データはソースによって特徴が全然違います。
特徴を知り、適切な前処理をしないと後で痛い目を見ます、、、

## ユーザー入力データ

Webフォームやアプリから得られる、もっとも“人間味”のあるデータ。

### 特徴

- テキスト、画像、動画、ファイルのアップロードなど
- ノイズが当たり前のように入ってくる
  - 想定外の入力（数字の部分にテキスト、絵文字の使用など）
  - 欠損や、表記揺れ
  - 意図していない改行、半角、全角
- バリデーションが十分でないことが多い
  - フロントエンドだけでチェックしているだけ

### 落とし穴

- 型が揺れる

```py
age = "25"  #（文字列）
age = 25    #（数値）
age = ""    #（空文字）
age = None  #（欠損）
```


## システム生成データ

ログ、イベント、メトリクス、機械的に吐き出されるデータ群。

### 特徴

- ボリュームが膨大（1日数GB〜TB）
- スキーマは比較的一貫している
- 自動生成されるため更新頻度が高い
- 時系列で蓄積される


### 落とし穴

- ログフォーマットの変更

```
{ "time": "2024-01-01T10:00:00Z", "user_id": 123 }
↓ 開発が気まぐれで key を変更
{ "timestamp": ... }
↓ 突然、解析のデータが０件に、、、
```

- タイムゾーン
  - UTC、JSTか不明
  - サマータイムで1時間ずれる → 分析が1日ずれに、、、
- イベント重複（二重カウント）
- ログ量が増えていくと処理時間も増えていく

## 内部データ（DB・社内API・自社サービス）

既存の業務システムが保持する“ビジネスロジック込み”のデータ。

### 特徴

- スキーマが明確
- データの意味が複雑（業務ロジックによる）
- 更新頻度やタイミングがシステムごとに異なる

### 落とし穴

- テーブルの意味を理解しないまま分析して誤解を生む
  - アクティブの意味が部署ごとに違う
  - 売り上げの計算方法が担当によって違う
- 本番だけNULLが来る
- ETLの同期タイミング差で不整合が起きる
  - 前日の明細に対して、ユーザー更新が最新


## サードバーティーデータ（外部API・購入データセット）

外部サービスや外部ベンダーから取得したデータ。

### 特徴

- 品質保証が弱い
- スキーマ変更・API停止のリスクあり
- 更新頻度が不定期

### 落とし穴

- 仕様通りにデータが来ないことも
- APIレート制限で処理が止まる
- 商用利用ライセンスに注意



