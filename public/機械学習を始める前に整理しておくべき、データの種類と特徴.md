---
title: 機械学習を始める前に整理しておくべき、データの種類と特徴
tags:
  - Data
  - 機械学習
private: false
updated_at: '2025-11-30T10:18:20+09:00'
id: 140c76f5d20231bc6b17
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

機械学習と聞くと、

・最新のモデルを使って予測する！
・いろんなアルゴリズムを試して精度を上げる！
・チャットボットで業務効率化！

などが思いつくかもしれません。

ただ、機械学習を学んでいく中で面白い格言と出会うでしょう。

> Garbage in, garbage out （ゴミを入れたら、ゴミが出てくる）
> コンピュータ分野でよく言われる格言です。

機械学習モデルというのは、学習するためにデータを使います。
そもそもそのデータの質が悪いと、モデルの質も悪くなるということも表しています。

どのアルゴリズムがいいか、どんな学習をさせたらいいかに目が行きがちですが、
学習させるデータも同じように大事ということです。

入力（学習）するデータは大事なのはわかったけど、どんな特徴があるんですか？
という方に向けてこの記事では全体像を整理しようと思います。

ざっくりデータってこんなものがあるのかと理解していただけると幸いです。


# 1. データソース

まず押さえておきたいのは、「どこから来たデータなのか」です。

データはソースによって特徴が全然違います。
特徴を知り、適切な前処理をしないと後で痛い目を見ます、、、

## ユーザー入力データ

Webフォームやアプリから得られる、もっとも“人間味”のあるデータ。

### 特徴

- テキスト、画像、動画、ファイルのアップロードなど
- ノイズが当たり前のように入ってくる
  - 想定外の入力（数字の部分にテキスト、絵文字の使用など）
  - 欠損や、表記揺れ
  - 意図していない改行、半角、全角
- バリデーションが十分でないことが多い
  - フロントエンドだけでチェックしているだけ

### 落とし穴

- 型が揺れる

```py
age = "25"  #（文字列）
age = 25    #（数値）
age = ""    #（空文字）
age = None  #（欠損）
```


## システム生成データ

ログ、イベント、メトリクス、機械的に吐き出されるデータ群。

### 特徴

- ボリュームが膨大（1日数GB〜TB）
- スキーマは比較的一貫している
- 自動生成されるため更新頻度が高い
- 時系列で蓄積される


### 落とし穴

- ログフォーマットの変更

```
{ "time": "2024-01-01T10:00:00Z", "user_id": 123 }
↓ 開発が気まぐれで key を変更
{ "timestamp": ... }
↓ 突然、解析のデータが０件に、、、
```

- タイムゾーン
  - UTC、JSTか不明
  - サマータイムで1時間ずれる → 分析が1日ずれに、、、
- イベント重複（二重カウント）
- ログ量が増えていくと処理時間も増えていく

## 内部データ（DB・社内API・自社サービス）

既存の業務システムが保持する“ビジネスロジック込み”のデータ。

### 特徴

- スキーマが明確
- データの意味が複雑（業務ロジックによる）
- 更新頻度やタイミングがシステムごとに異なる

### 落とし穴

- テーブルの意味を理解しないまま分析して誤解を生む
  - アクティブの意味が部署ごとに違う
  - 売り上げの計算方法が担当によって違う
- 本番だけNULLが来る
- ETLの同期タイミング差で不整合が起きる
  - 前日の明細に対して、ユーザー更新が最新


## サードバーティーデータ（外部API・購入データセット）

外部サービスや外部ベンダーから取得したデータ。

### 特徴

- 品質保証が弱い
- スキーマ変更・API停止のリスクあり
- 更新頻度が不定期

### 落とし穴

- 仕様通りにデータが来ないことも
- APIレート制限で処理が止まる
- 商用利用ライセンスに注意


# 2. データフォーマット

データが「どんな形」で保存されているかは、
前処理・ETL・学習・分析のしやすさに直結します。

この章ではよく使われるフォーマットについて整理していきます。

## データシリアライズ

まず前提として、データはそのままでは保存や転送ができません。

そこで必要になるのが **シリアライズ（Serialization）**です。

> オブジェクトを保存・転送可能な形式に変換すること。

逆に元のオブジェクトに戻すことを デシリアライズといいます。

よく使われるシリアライズ形式は次の表です。

形式 | 特徴
 -- | -- 
 JSON | 人間にも読みやすい。APIでもっとも一般的。
 CSV | 表形式の王道フォーマット。
 Parquet | 列志向で分析に適している。
 Protobuf / Avro | 高速・高圧縮のバイナリ形式

というわけで、よく使われるJSON, CSV, Parquetを詳しく見ていきます。

## JSON

API、ログ、イベント…現場で最もよく見る形式。

```json
{
  "student_id": "B005",
  "name": "山田 花子",
  "grades": [
    {
      "subject": "数学",
      "score": 85,
      "credit": 4
    },
    {
      "subject": "英語",
      "score": 92,
      "credit": 4
    },
  ],
  "average_score": 85.0
}
```

### 特徴

- 可読性が高い
- ネスト構造への対応が柔軟
- セミ構造化データとして扱いやすい


### 落とし穴

- 型が揺れる

```py
price: "1000"（文字列）
price: 1000（数値）
price: null

# → pandasの読み込みで型がobjectになり処理が破綻。。。
```

- ネストが深すぎると扱いづらい

```py
order.user.address.city

# → カラム展開（flatten）に時間がかかり、ETLが複雑化。
```

- キーが日によって微妙に違う
  - snake_caseとcamelCaseが混在
  - 任意フィールドが存在したりしなかったり
  - リグのバージョン違いが混ざる


## CSV

データ分析・ETLで最も古く、最も使われるフォーマット。

```csv
学籍番号,氏名,数学,英語
A001,佐藤 太郎,85,92
A002,山田 花子,78,88
A003,田中 健太,90,75
```

```csv
日付,場所,気温(℃),湿度(%),天気
2025-11-20,東京,15.5,60,晴れ
2025-11-20,大阪,18.0,55,くもり
2025-11-21,東京,12.3,72,雨
```


### 特徴

- どんなツールでも扱える
- 形容で汎用的
- 人間が直接開ける


### 落とし穴

- 値の中にカンマが含まれる

```csv
"Tokyo, Japan", 39

<!-- → ただのsplit(”,”)で壊れる -->
```

- 文字コード問題
  - Windows: Shift-JIS
  - Mac/Linux: UTF-8
  - 日本企業でありがちな定番トラブル

- 型がない
  - 全て文字列なので、日付/数値の再変換が必須。
  - 変換ミス（"2024-1-1"など）による事故が多い。

- 空白・欠損の表現がばらばら
	- 空文字 “”
	- NULL
	- NaN
	- 区切りだけ ,,


## Parquet

データレイク・大規模分析で必須。

※ バイナリデータのため、人が見ても理解ができません、、、

### 特徴

- 列志向
- 圧縮率が高い
- 部分読み出しが高速
- Spark、BigQuery、Athena などと相性抜群

> 列志向とは？
>
> 従来の行志向（CSVや一般的なデータベース）とは異なり、データを行ごとではなく列ごとにまとめて保存するデータ格納方式のこと。
> クエリの高速化や高いデータ圧縮率といったメリットがある。


### 落とし穴

- 列志向のメリットを理解していない
  - 必要な列だけを読むのは得意
  - データ型が保持される
  - → 逆に理解せずに使うと性能が出ない
    - 例えば、すべての列を取得するなど、、、


