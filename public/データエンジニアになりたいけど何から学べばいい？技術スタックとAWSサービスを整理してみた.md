---
title: データエンジニアになりたいけど何から学べばいい？技術スタックとAWSサービスを整理してみた
tags:
  - AWS
  - データエンジニア
private: false
updated_at: '2025-12-13T11:47:35+09:00'
id: 911c7c9a2daa80029109
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

「データエンジニアになりたい！」と思って学習を始めようとしたものの、
調べれば調べるほど技術スタックが多すぎて何から手をつければいいのか分からなくなりました。

Python、SQL、Spark、Airflow、AWS、GCP...挙げればキリがありません。

そこで、データエンジニアリングに必要なスキルとAWSのサービスを体系的に整理してみました。
同じように「何を学べばいいの？」と迷っている方の参考になれば幸いです。

> データエンジニアリングとは、、、
>
> **データの収集、保管、変換、および提供**を支えるパイプラインとインフラストラクチャの設計・構築・保守に焦点を当てた分野

# 必要なスキル

## a. プログラミングとスクリプト

データ処理とパイプライン構築の核となるスキル

- **Python**: データ操作（Pandasなど）、ETL/ELTパイプライン構築、クラウドサービスとの連携、API開発などで最も広く使われる。
- **SQL**: データベースの操作、データ抽出、変換を行う上で**最も重要**なスキルの一つ。複雑なクエリの作成、パフォーマンスチューニングが必要。
- **シェルスクリプト（Bash/Shell）**: サーバー環境での自動化、ファイル操作、ジョブのスケジューリングなどに必要。

## b. データベース技術

データの保管とアクセス方法を決定する重要なスキル

- **リレーショナルデータベース (RDBMS)**:
    - PostgreSQL, MySQL, Oracleなどの理解。
    - 正規化、インデックス、トランザクション、クエリ最適化の知識。
- **NoSQLデータベース**:
    - MongoDB, Cassandra, Redisなどの特性と適切な使用ケースの理解。
- **データウェアハウス (DWH)**:
    - Snowflake, Amazon Redshift, Google BigQueryなどのクラウドDWHの使用経験。
    - DWH特有のスキーマ設計（スタースキーマ、スノーフレイクスキーマ）の知識。

## c. クラウドコンピューティング

現代のデータインフラはクラウド上に構築するのが主流

- **主要なプラットフォーム**: **AWS, Google Cloud (GCP), Azure**のいずれか。
- **重要なサービス**:
    - **ストレージ**: S3 (AWS), Cloud Storage (GCP)
    - **コンピューティング**: EC2 (AWS), Compute Engine (GCP), Lambda/Cloud Functions
    - **データベース/DWH**: Redshift (AWS), BigQuery (GCP), RDS/Cloud SQL
    - **データパイプライン**: AWS Glue, Cloud Dataflow, Airflow (マネージドサービス含む)

## d. データパイプラインとワークフロー管理

データの流れを定義し、自動化し、監視するためのツールと概念

- **ETL/ELTツールの知識**: データ抽出 (Extract)、変換 (Transform)、ロード (Load) の概念。
- **ワークフローオーケストレーション**:
    - **Apache Airflow**: ジョブの依存関係管理、スケジューリング、監視を行うためのデファクトスタンダードツール。
    - AWS Step Functions, GCP Cloud Composer/Workflowsなどのクラウドサービス。

## e. ビッグデータ技術

大量のデータを効率的に処理するためのフレームワーク

- **Apache Spark**: 大規模なデータ処理と分析を行うための高速な分散処理フレームワーク。
- **Apache Kafka**: ストリーミングデータ処理（リアルタイムデータ処理）のための分散メッセージングシステム。

# AWSデータエンジニアリング関連サービス

データエンジニアリングのライフサイクル（収集→保存→処理→分析→カタログ/ガバナンス）に沿って分類してみます。

| **カテゴリ** | **目的** | **主要サービス** |
| --- | --- | --- |
| **1. ストレージ（Data Lake Core）** | 大量の生データおよび処理済みデータを保存する。 | **Amazon S3** |
| **2. データ処理・変換（ETL/ELT）** | 生データを分析可能な形に変換（Transform）する。 | **AWS Glue**, **Amazon EMR**, **AWS Lambda** |
| **3. データウェアハウス・分析（Serving）** | 構造化されたデータを集約し、高速な分析を提供する。 | **Amazon Redshift**, **Amazon Athena** |
| **4. パイプライン管理・オーケストレーション** | 処理ジョブの実行順序、依存関係を管理し、自動化する。 | **AWS Step Functions**, **Apache Airflow on Amazon MWAA** |
| **5. ストリーミング・リアルタイム処理** | リアルタイムで生成されるデータを処理し、低遅延で利用可能にする。 | **Amazon Kinesis Data Streams**, **Amazon Kinesis Data Firehose**, **Amazon Managed Streaming for Apache Kafka (MSK)** |
| **6. カタログ・ガバナンス** | データのメタデータ（スキーマ）を管理し、アクセス制御を一元化する。 | **AWS Glue Data Catalog**, **Amazon Lake Formation** |

## 1. ストレージ：Amazon S3 (Simple Storage Service)

AWSにおける**データレイクの中心**となるオブジェクトストレージサービス。

- **特徴**: 無制限の拡張性、高い耐久性、低コスト。
- **役割**: あらゆる種類の生データ、中間処理データ、最終的な処理済みデータを保存できる。データエンジニアは、S3のパーティショニング設計やストレージクラスの管理（コスト最適化）を行う。

## 2. データ処理エンジン

S3に格納されたデータを処理・変換するためのサービス群。

| **サービス名** | **概要** | **適したユースケース** |
| --- | --- | --- |
| **AWS Glue** | フルマネージドな**サーバーレス** ETLサービス。Apache Sparkベースで、Glue Data Catalogと連携。 | サーバーレスで手軽に大規模なデータ変換パイプラインを構築したい場合。 |
| **Amazon EMR (Elastic MapReduce)** | Apache Spark, Hive, Prestoなどのビッグデータフレームワークを簡単にデプロイ・管理できるマネージドクラスターサービス。 | **カスタマイズ性や柔軟性が高い**大規模な分散処理、既存のHadoopエコシステム資産を活用したい場合。 |
| **AWS Lambda** | サーバーレスなイベント駆動型コンピューティングサービス。 | ファイルのS3への到着をトリガーとした**小規模なデータ処理**や、ETLジョブの開始トリガーなど。 |

## 3. データウェアハウスと分析

| **サービス名** | **概要** | **適したユースケース** |
| --- | --- | --- |
| **Amazon Redshift** | Petabyte規模の構造化データを高速に分析するための**データウェアハウス**。列指向ストレージを使用。 | **BIツール**や定型レポート作成のための、高速で複雑なSQLクエリを頻繁に実行する場合。 |
| **Amazon Athena** | **サーバーレス**なクエリサービス。S3上のデータに対して標準SQLで直接クエリを実行できる。 | 一時的・アドホックな分析、データレイクの探索。クエリした分だけ課金される。 |

## 4. オーケストレーションとワークフロー管理

- **AWS Step Functions**
  - 複数のAWSサービス（Glue, Lambda, SageMakerなど）を連携させた、複雑なワークフローを視覚的に定義し、実行・管理するサービス。
  - データの流れに**条件分岐**や**エラー処理**を組み込むのに最適。
- **Amazon Managed Workflows for Apache Airflow (MWAA)**
  - 人気のワークフロー管理ツールであるApache AirflowをAWS上でフルマネージドで利用できるサービス。
  - 複雑な依存関係を持つパイプラインの管理に優れている。

## 5. ストリーミングとリアルタイム処理

リアルタイムデータの取り込みと処理を支える。

- **Amazon Kinesis (Data Streams / Data Firehose)**:
    - **Data Streams**: リアルタイムデータの永続的な保存と処理（カスタムコンシューマー開発が必要）。
    - **Data Firehose**: 既存の送信先（S3、Redshift、Elasticsearchなど）に自動でデータをロードするマネージドサービス。
- **Amazon MSK (Managed Streaming for Apache Kafka)**: Apache KafkaクラスターをAWS上で簡単にセットアップ・運用できるサービス。

## 6. データガバナンスとカタログ

- **AWS Glue Data Catalog**
  - S3などのストレージにあるデータの**メタデータ（スキーマ、場所など）を一元管理**するカタログサービス。
  - AthenaやRedshift Spectrum、GlueなどのAWSサービスがS3上のデータを理解するために不可欠。
- **Amazon Lake Formation**
  - データレイク（S3）に対する**セキュリティとアクセス制御**を一元的に設定・管理するためのサービス。
  - 誰がどのS3のデータにアクセスできるかを、データベース権限のように設定できる。

